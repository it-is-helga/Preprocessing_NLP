{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b262b75c",
   "metadata": {},
   "source": [
    "# NLP Lab â€“ Minimal Preprocessing Experiments (Single Task)\n",
    "\n",
    "This notebook runs a tiny grid of preprocessing experiments on **one** supervised text classification task\n",
    "and saves results to `results_single_task.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96259f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import make_scorer, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0aad0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "OUTFILE = Path(\"results_single_task_3_classes.csv\")\n",
    "\n",
    "macro_f1 = make_scorer(f1_score, average=\"macro\")\n",
    "SCORING = {\"acc\": \"accuracy\", \"macro_f1\": macro_f1}\n",
    "\n",
    "def mean_scores(scores):\n",
    "    return {k.replace(\"test_\", \"\"): float(np.mean(v))\n",
    "            for k, v in scores.items() if k.startswith(\"test_\")}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45b8320f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stopwordsiso in /home/ceres/anaconda3/lib/python3.10/site-packages (0.6.1)\n",
      "Prep: DON\n",
      "I am travelling to Nancy for an NLP course at IDMC :https://idmc.univ-lorraine.fr/\n",
      "Prep: LOW\n",
      "i am travelling to nancy for an nlp course at idmc :https://idmc.univ-lorraine.fr/\n",
      "Prep: URLrem\n",
      "I am travelling to Nancy for an NLP course at IDMC : \n",
      "Prep: URLrep\n",
      "I am travelling to Nancy for an NLP course at IDMC : <URL> \n",
      "Prep: PUN\n",
      "I am travelling to Nancy for an NLP course at IDMC  https   idmc univ lorraine fr \n",
      "Prep: RSW\n",
      "I travelling Nancy NLP IDMC : https : / / idmc . univ - lorraine . /\n",
      "Prep: LOW+URLrem\n",
      "i am travelling to nancy for an nlp course at idmc :\n",
      "Prep: LOW+URLrep\n",
      "i am travelling to nancy for an nlp course at idmc : <URL>\n",
      "Prep: LOW+PUN\n",
      "i am travelling to nancy for an nlp course at idmc https idmc univ lorraine fr\n",
      "Prep: LOW+URLrem+PUN\n",
      "i am travelling to nancy for an nlp course at idmc\n",
      "Prep: LOW+URLrep+PUN\n",
      "i am travelling to nancy for an nlp course at idmc\n",
      "Prep: LOW+URLrem+PUN+RSW\n",
      "travelling nancy nlp idmc\n",
      "Prep: LOW+URLrep+PUN+RSW\n",
      "travelling nancy nlp idmc URL\n"
     ]
    }
   ],
   "source": [
    "# --- Preprocessing (kept simple) ---\n",
    "\n",
    "!pip install stopwordsiso\n",
    "import stopwordsiso\n",
    "\n",
    "def don(text): \n",
    "    return text\n",
    "    \n",
    "def lower(text): \n",
    "    return text.lower()\n",
    "\n",
    "URL_RE = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "def remove_urls(text): \n",
    "    return URL_RE.sub(\" \", text)\n",
    "    \n",
    "def replace_urls(text): \n",
    "    return URL_RE.sub(\" <URL> \", text)\n",
    "    \n",
    "PUNCT_RE = re.compile(r\"[^\\w\\s]\")\n",
    "def remove_punct(text): \n",
    "    return PUNCT_RE.sub(\" \", text)\n",
    "\n",
    "TOKEN_PUNC = re.compile(r\"\\w+|[^\\w\\s]\")\n",
    "def remove_stopwords(text):\n",
    "    #very slow but keeps punctuation\n",
    "    \n",
    "    return \" \".join([T for T in TOKEN_PUNC.findall(text) if T not in set(stopwordsiso.stopwords(\"en\"))])\n",
    "\n",
    "    \n",
    "def compose(*funcs):\n",
    "    def f(text):\n",
    "        for fn in funcs:\n",
    "            text = fn(text)\n",
    "        return re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return f\n",
    "\n",
    "PREPROCESSORS = {\n",
    "    \"DON\": don,\n",
    "    \"LOW\": lower,\n",
    "    \"URLrem\": remove_urls,\n",
    "    \"URLrep\": replace_urls,\n",
    "    \"PUN\": remove_punct,\n",
    "    \"RSW\": remove_stopwords,\n",
    "    \"LOW+URLrem\": compose(lower, remove_urls),\n",
    "    \"LOW+URLrep\": compose(lower, replace_urls),\n",
    "    \"LOW+PUN\": compose(lower, remove_punct),\n",
    "    \"LOW+URLrem+PUN\": compose(lower, remove_urls, remove_punct),\n",
    "    \"LOW+URLrep+PUN\": compose(lower, remove_urls, remove_punct),\n",
    "    \"LOW+URLrem+PUN+RSW\": compose(lower, remove_urls, remove_punct, remove_stopwords),\n",
    "    \"LOW+URLrep+PUN+RSW\": compose(lower, replace_urls, remove_punct, remove_stopwords),\n",
    "\n",
    "}\n",
    "toto = \"I am travelling to Nancy for an NLP course at IDMC :https://idmc.univ-lorraine.fr/\" \n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    print(f\"Prep: {prep_name}\")\n",
    "    print(prep(toto))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f18f0c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "Samples: 2950 Classes: {0, 1, 2} Labels: ['comp.graphics', 'sci.med', 'sci.space']\n"
     ]
    }
   ],
   "source": [
    "# 20 news groups to test, 3 will be enough for our purpose\n",
    "# ref : https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_20newsgroups.html\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')\n",
    "print(list(newsgroups_train.target_names))\n",
    "print(\"\")\n",
    "cats = [\"comp.graphics\", \"sci.space\",\"sci.med\"]\n",
    "data = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    categories=cats,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "X, y = data.data, data.target\n",
    "print(\"Samples:\", len(X), \"Classes:\", set(y), \"Labels:\", data.target_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a0118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = LogisticRegression(max_iter=2000, random_state=SEED)\n",
    "\n",
    "def evaluate(X, y, preprocess, vectorizer):\n",
    "    Xp = [preprocess(t) for t in X]\n",
    "    pipe = Pipeline([\n",
    "        (\"vect\", vectorizer),\n",
    "        (\"clf\", MODEL),\n",
    "    ])\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    scores = cross_validate(pipe, Xp, y, cv=cv, scoring=SCORING, n_jobs=-1)\n",
    "    return mean_scores(scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4af70d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prep: DON          | Vec: count_word_1-1\n",
      "{'acc': 0.8522033898305084, 'macro_f1': 0.852437380580561}\n",
      "Prep: DON          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8884745762711864, 'macro_f1': 0.8887987585640236}\n",
      "Prep: DON          | Vec: count_char_3-5\n",
      "{'acc': 0.8508474576271187, 'macro_f1': 0.8511190071050786}\n",
      "Prep: DON          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8722033898305085, 'macro_f1': 0.8723882274002837}\n",
      "Prep: LOW          | Vec: count_word_1-1\n",
      "{'acc': 0.8589830508474577, 'macro_f1': 0.8591981684158906}\n",
      "Prep: LOW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.892542372881356, 'macro_f1': 0.8928052469927857}\n",
      "Prep: LOW          | Vec: count_char_3-5\n",
      "{'acc': 0.8477966101694914, 'macro_f1': 0.8480633280430425}\n",
      "Prep: LOW          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8745762711864407, 'macro_f1': 0.8747612373904516}\n",
      "Prep: URL          | Vec: count_word_1-1\n",
      "{'acc': 0.851186440677966, 'macro_f1': 0.851427555541919}\n",
      "Prep: URL          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.887457627118644, 'macro_f1': 0.8877747632381047}\n",
      "Prep: URL          | Vec: count_char_3-5\n",
      "{'acc': 0.8515254237288137, 'macro_f1': 0.8518091911947758}\n",
      "Prep: URL          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8718644067796608, 'macro_f1': 0.8720513049296775}\n",
      "Prep: PUN          | Vec: count_word_1-1\n",
      "{'acc': 0.8522033898305084, 'macro_f1': 0.852437380580561}\n",
      "Prep: PUN          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.8884745762711864, 'macro_f1': 0.8887987585640236}\n",
      "Prep: PUN          | Vec: count_char_3-5\n",
      "{'acc': 0.8562711864406779, 'macro_f1': 0.8564602859797732}\n",
      "Prep: PUN          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8918644067796612, 'macro_f1': 0.8920751878224585}\n",
      "Prep: RSW          | Vec: count_word_1-1\n",
      "{'acc': 0.8667796610169493, 'macro_f1': 0.8670595390840343}\n",
      "Prep: RSW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.9054237288135594, 'macro_f1': 0.9056561175530273}\n",
      "Prep: RSW          | Vec: count_char_3-5\n",
      "{'acc': 0.8576271186440678, 'macro_f1': 0.8578325576838279}\n",
      "Prep: RSW          | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8786440677966102, 'macro_f1': 0.8786059224559931}\n",
      "Prep: LOW+URL      | Vec: count_word_1-1\n",
      "{'acc': 0.8593220338983052, 'macro_f1': 0.8595353411414284}\n",
      "Prep: LOW+URL      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.892542372881356, 'macro_f1': 0.8928007919905989}\n",
      "Prep: LOW+URL      | Vec: count_char_3-5\n",
      "{'acc': 0.8467796610169491, 'macro_f1': 0.8471340404084821}\n",
      "Prep: LOW+URL      | Vec: tfidf_char_3-5\n",
      "{'acc': 0.8776271186440677, 'macro_f1': 0.8777751014770002}\n",
      "Prep: LOW+PUN      | Vec: count_word_1-1\n",
      "{'acc': 0.8589830508474577, 'macro_f1': 0.8591981684158906}\n",
      "Prep: LOW+PUN      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.892542372881356, 'macro_f1': 0.8928052469927857}\n",
      "Prep: LOW+PUN      | Vec: count_char_3-5\n",
      "{'acc': 0.8535593220338983, 'macro_f1': 0.8538927851640956}\n",
      "Prep: LOW+PUN      | Vec: tfidf_char_3-5\n",
      "{'acc': 0.891186440677966, 'macro_f1': 0.8913932063681553}\n",
      "Prep: LOW+URL+PUN  | Vec: count_word_1-1\n",
      "{'acc': 0.8593220338983052, 'macro_f1': 0.8595353411414284}\n",
      "Prep: LOW+URL+PUN  | Vec: tfidf_word_1-1\n",
      "{'acc': 0.892542372881356, 'macro_f1': 0.8928007919905989}\n",
      "Prep: LOW+URL+PUN  | Vec: count_char_3-5\n",
      "{'acc': 0.8542372881355933, 'macro_f1': 0.8546141667150673}\n",
      "Prep: LOW+URL+PUN  | Vec: tfidf_char_3-5\n",
      "{'acc': 0.891864406779661, 'macro_f1': 0.8920937258835867}\n",
      "Prep: LOW+URL+PUN+RSW | Vec: count_word_1-1\n",
      "{'acc': 0.8759322033898304, 'macro_f1': 0.8762009715847908}\n",
      "Prep: LOW+URL+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.9105084745762712, 'macro_f1': 0.9106883487101041}\n",
      "Prep: LOW+URL+PUN+RSW | Vec: count_char_3-5\n",
      "{'acc': 0.8654237288135593, 'macro_f1': 0.8656927519247135}\n",
      "Prep: LOW+URL+PUN+RSW | Vec: tfidf_char_3-5\n",
      "{'acc': 0.9040677966101696, 'macro_f1': 0.904220733579519}\n"
     ]
    }
   ],
   "source": [
    "VECTORIZERS = {\n",
    "    \"count_word_1-1\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"count_word_1-2\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"tfidf_word_1-1\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"tfidf_word_1-2\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"count_char_3-5\": CountVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"count_charwb_3-5\": CountVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "    \"tfidf_char_3-5\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"tfidf_charwb_3-5\": TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    for vec_name, vec in VECTORIZERS.items():\n",
    "        print(f\"Prep: {prep_name:12s} | Vec: {vec_name}\")\n",
    "        res = evaluate(X, y, prep, vec)\n",
    "        print(res)\n",
    "        rows.append({\n",
    "            \"preprocessing\": prep_name,\n",
    "            \"vectorizer\": vec_name,\n",
    "            **res\n",
    "        })\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fe8d0203-210d-4471-928c-4064db7d2d77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      preprocessing      vectorizer       acc  macro_f1\n",
      "33  LOW+URL+PUN+RSW  tfidf_word_1-1  0.910508  0.910688\n",
      "17              RSW  tfidf_word_1-1  0.905424  0.905656\n",
      "35  LOW+URL+PUN+RSW  tfidf_char_3-5  0.904068  0.904221\n",
      "25          LOW+PUN  tfidf_word_1-1  0.892542  0.892805\n",
      "5               LOW  tfidf_word_1-1  0.892542  0.892805\n",
      "21          LOW+URL  tfidf_word_1-1  0.892542  0.892801\n",
      "29      LOW+URL+PUN  tfidf_word_1-1  0.892542  0.892801\n",
      "31      LOW+URL+PUN  tfidf_char_3-5  0.891864  0.892094\n",
      "15              PUN  tfidf_char_3-5  0.891864  0.892075\n",
      "27          LOW+PUN  tfidf_char_3-5  0.891186  0.891393\n",
      "13              PUN  tfidf_word_1-1  0.888475  0.888799\n",
      "1               DON  tfidf_word_1-1  0.888475  0.888799\n",
      "9               URL  tfidf_word_1-1  0.887458  0.887775\n",
      "19              RSW  tfidf_char_3-5  0.878644  0.878606\n",
      "23          LOW+URL  tfidf_char_3-5  0.877627  0.877775\n",
      "32  LOW+URL+PUN+RSW  count_word_1-1  0.875932  0.876201\n",
      "7               LOW  tfidf_char_3-5  0.874576  0.874761\n",
      "3               DON  tfidf_char_3-5  0.872203  0.872388\n",
      "11              URL  tfidf_char_3-5  0.871864  0.872051\n",
      "16              RSW  count_word_1-1  0.866780  0.867060\n",
      "34  LOW+URL+PUN+RSW  count_char_3-5  0.865424  0.865693\n",
      "20          LOW+URL  count_word_1-1  0.859322  0.859535\n",
      "28      LOW+URL+PUN  count_word_1-1  0.859322  0.859535\n",
      "4               LOW  count_word_1-1  0.858983  0.859198\n",
      "24          LOW+PUN  count_word_1-1  0.858983  0.859198\n",
      "18              RSW  count_char_3-5  0.857627  0.857833\n",
      "14              PUN  count_char_3-5  0.856271  0.856460\n",
      "30      LOW+URL+PUN  count_char_3-5  0.854237  0.854614\n",
      "26          LOW+PUN  count_char_3-5  0.853559  0.853893\n",
      "12              PUN  count_word_1-1  0.852203  0.852437\n",
      "0               DON  count_word_1-1  0.852203  0.852437\n",
      "10              URL  count_char_3-5  0.851525  0.851809\n",
      "8               URL  count_word_1-1  0.851186  0.851428\n",
      "2               DON  count_char_3-5  0.850847  0.851119\n",
      "6               LOW  count_char_3-5  0.847797  0.848063\n",
      "22          LOW+URL  count_char_3-5  0.846780  0.847134\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows).sort_values(\"macro_f1\", ascending=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cb4790cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to: results_single_task.csv\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(OUTFILE, index=False)\n",
    "print(f\"Saved to: {OUTFILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24da9072-5156-40fa-9fda-c4c3048f8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stopwordsiso\n",
      "  Downloading stopwordsiso-0.6.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading stopwordsiso-0.6.1-py3-none-any.whl (73 kB)\n",
      "Installing collected packages: stopwordsiso\n",
      "Successfully installed stopwordsiso-0.6.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ceres/anaconda3/lib/python3.10/site-packages/stopwordsiso/_core.py:4: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16ac99a6-74fe-44d7-b7b3-2cf8ae2636cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples: 18846 Classes: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19} Labels: ['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "Prep: DON          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7203642728302155, 'macro_f1': 0.7095112650794365}\n",
      "Prep: LOW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7311891016550626, 'macro_f1': 0.7195677702933339}\n",
      "Prep: URL          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7207887886168962, 'macro_f1': 0.7101215641322157}\n",
      "Prep: PUN          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7203642728302155, 'macro_f1': 0.7095112650794365}\n",
      "Prep: RSW          | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7362298606600122, 'macro_f1': 0.7258475730738727}\n",
      "Prep: LOW+URL      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7310299363859716, 'macro_f1': 0.7196131318136788}\n",
      "Prep: LOW+PUN      | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7311891016550626, 'macro_f1': 0.7195677702933339}\n",
      "Prep: LOW+URL+PUN  | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7310299363859716, 'macro_f1': 0.7196131318136788}\n",
      "Prep: LOW+URL+PUN+RSW | Vec: tfidf_word_1-1\n",
      "{'acc': 0.7451443262184243, 'macro_f1': 0.7349656336750879}\n"
     ]
    }
   ],
   "source": [
    "full_data = fetch_20newsgroups(\n",
    "    subset=\"all\",\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    ")\n",
    "X, y = full_data.data, full_data.target\n",
    "print(\"Samples:\", len(X), \"Classes:\", set(y), \"Labels:\", full_data.target_names)\n",
    "\n",
    "VECTORIZERS = {\n",
    "    #\"count_word_1-1\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"count_word_1-2\": CountVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    \"tfidf_word_1-1\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 1), lowercase=False),\n",
    "    #\"tfidf_word_1-2\": TfidfVectorizer(analyzer=\"word\", ngram_range=(1, 2), lowercase=False),\n",
    "    #\"count_char_3-5\": CountVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"count_charwb_3-5\": CountVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "    \"tfidf_char_3-5\": TfidfVectorizer(analyzer=\"char\", ngram_range=(3, 5), lowercase=False),\n",
    "    #\"tfidf_charwb_3-5\": TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(3, 5), lowercase=False),\n",
    "}\n",
    "\n",
    "rows_full = []\n",
    "for prep_name, prep in PREPROCESSORS.items():\n",
    "    for vec_name, vec in VECTORIZERS.items():\n",
    "        print(f\"Prep: {prep_name:12s} | Vec: {vec_name}\")\n",
    "        res = evaluate(X, y, prep, vec)\n",
    "        print(res)\n",
    "        rows_full.append({\n",
    "            \"preprocessing\": prep_name,\n",
    "            \"vectorizer\": vec_name,\n",
    "            **res\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50a3cb66-2ec4-45c7-96da-5926c19501c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     preprocessing      vectorizer       acc  macro_f1\n",
      "8  LOW+URL+PUN+RSW  tfidf_word_1-1  0.745144  0.734966\n",
      "4              RSW  tfidf_word_1-1  0.736230  0.725848\n",
      "5          LOW+URL  tfidf_word_1-1  0.731030  0.719613\n",
      "7      LOW+URL+PUN  tfidf_word_1-1  0.731030  0.719613\n",
      "1              LOW  tfidf_word_1-1  0.731189  0.719568\n",
      "6          LOW+PUN  tfidf_word_1-1  0.731189  0.719568\n",
      "2              URL  tfidf_word_1-1  0.720789  0.710122\n",
      "0              DON  tfidf_word_1-1  0.720364  0.709511\n",
      "3              PUN  tfidf_word_1-1  0.720364  0.709511\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(rows_full).sort_values(\"macro_f1\", ascending=False)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8ebb05-c365-450d-8574-ffb44f56d5f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
